---
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 75-worker-sysctl-elasticsearch
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - contents:
          source: data:text/plain;charset=utf-8;base64,dm0ubWF4X21hcF9jb3VudCA9IDI2MjE0NAo=
        mode: 420
        overwrite: true
        path: /etc/sysctl.d/vm_max_map_count.conf

# =============================================================================
# STEP 2: Create ECK namespace
# =============================================================================

---
apiVersion: v1
kind: Namespace
metadata:
  name: elastic-observability
  labels:
    name: elastic-observability


---
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: observability-cluster
  namespace: elastic-observability
spec:
  version: 9.1.2
  
  # =============================================================================
  # Master + Data Node on worker01
  # =============================================================================
  nodeSets:
  - name: data-worker01
    count: 1
    config:
      # Node roles: master + data for high availability
      node.roles: ["master", "data_content", "data_hot", "ingest"]
      
      # Performance settings
      node.store.allow_mmap: false  # Required for OpenShift restricted SCC
      
      # Observability-specific settings
      indices.memory.index_buffer_size: "20%"
      cluster.routing.allocation.disk.threshold_enabled: true
      cluster.routing.allocation.disk.watermark.low: "85%"
      cluster.routing.allocation.disk.watermark.high: "90%"
      cluster.routing.allocation.disk.watermark.flood_stage: "95%"
      
      # Data tier settings
      node.attr.data_tier: "data_hot"
      cluster.routing.allocation.awareness.attributes: "data_tier"
      
      # Index lifecycle management      
    podTemplate:
      metadata:
        labels:
          workload-type: observability-data
      spec:        
        # Resource allocation - utilizing most of worker01 resources
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: "16Gi"
              cpu: "6000m"
            limits:
              memory: "16Gi"
              cpu: "6000m"
          env:
          # JVM heap size (75% of container memory)
          - name: ES_JAVA_OPTS
            value: "-Xms12g -Xmx12g"
          
    # Storage using LVMS from 1TB /dev/vdb
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 400Gi  # Using 400GB of the 1TB available
        storageClassName: "lvms-vdb-storage"
        
  # =============================================================================
  # Master + Data Node on worker02
  # =============================================================================
  - name: data-worker02
    count: 1
    config:
      # Node roles: master + data for high availability
      node.roles: ["master", "data_content", "data_hot", "ingest"]
      
      # Performance settings
      node.store.allow_mmap: false
      
      # Observability-specific settings (same as worker01)
      indices.memory.index_buffer_size: "20%"
      cluster.routing.allocation.disk.threshold_enabled: true
      cluster.routing.allocation.disk.watermark.low: "85%"
      cluster.routing.allocation.disk.watermark.high: "90%"
      cluster.routing.allocation.disk.watermark.flood_stage: "95%"
      
      # Data tier settings
      node.attr.data_tier: "data_hot"
      cluster.routing.allocation.awareness.attributes: "data_tier"
            
    podTemplate:
      metadata:
        labels:
          workload-type: observability-data
      spec:        
        # Resource allocation - utilizing most of worker02 resources
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: "16Gi"
              cpu: "6000m"
            limits:
              memory: "16Gi"
              cpu: "6000m"
          env:
          # JVM heap size (75% of container memory)
          - name: ES_JAVA_OPTS
            value: "-Xms12g -Xmx12g"
          
    # Storage using LVMS from 1TB /dev/vdb
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 400Gi  # Using 400GB of the 1TB available
        storageClassName: "lvms-vdb-storage"

  # =============================================================================
  # Master + Machine Learning Node on worker03
  # =============================================================================
  - name: ml-worker03
    count: 1
    config:
      # Machine learning + master node roles
      node.roles: ["master", "ml", "remote_cluster_client"]
      
      # ML-specific settings
      node.store.allow_mmap: false
      
      # ML memory settings
      xpack.ml.max_machine_memory_percent: 70  # Conservative for shared worker03
      xpack.ml.use_auto_machine_memory_percent: true
      
      # ML job settings
      xpack.ml.max_anomaly_records: 500
      xpack.ml.max_open_jobs: 15  # Reduced for shared resources
      
    podTemplate:
      metadata:
        labels:
          workload-type: machine-learning
      spec:        
        # Resource allocation for ML node (sharing worker03 with Kibana, Fleet, AAP)
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: "8Gi"
              cpu: "3000m"
            limits:
              memory: "8Gi"
              cpu: "3000m"
          env:
          # JVM heap size for ML node
          - name: ES_JAVA_OPTS
            value: "-Xms6g -Xmx6g"
          
    # Storage for ML node
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 200Gi  # Using 200GB for ML node data
        storageClassName: "lvms-vdb-storage"

  # HTTP Service Configuration
  http:
    tls:
      selfSignedCertificate:
        disabled: false
    service:
      spec:
        type: ClusterIP


====
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: observability-kibana
  namespace: elastic-observability
spec:
  config:
    xpack.fleet.agents.fleet_server.hosts: ["https://fleet-server-agent-http.elastic-observability.svc.cluster.local:8220"]
    xpack.fleet.packages:
    - name: system
      version: latest
    - name: elastic_agent
      version: latest
    - name: fleet_server
      version: latest
    - name: kubernetes
      version: latest
    xpack.fleet.agentPolicies:
    # Fleet server policy
    - id: eck-fleet-server
      monitoring_enabled:
      - logs
      - metrics
      name: Fleet Server on ECK policy
      namespace: default
      package_policies:
      - id: fleet_server-1
        name: fleet_server-1
        package:
          name: fleet_server
      unenroll_timeout: 900
    
    # Simplified agent policy - let Fleet manage package details
    - id: eck-agent
      monitoring_enabled:
      - logs
      - metrics
      name: Elastic Agent on ECK policy
      namespace: default
      is_default: true
      unenroll_timeout: 900
      package_policies:
      - package:
          name: system
        name: system-1
        # Use default system package configuration
      - package:
          name: kubernetes
        name: kubernetes-1
        # Use default kubernetes package configuration
        # ECK will handle proper service account and permissions
        
  version: 9.1.2
  count: 1
  elasticsearchRef:
    name: observability-cluster
  podTemplate:
    metadata:
      labels:
        workload-type: observability-ui
    spec:
      # Node selector for worker03
      nodeSelector:
        kubernetes.io/hostname: "worker03.ocp4.example.com"
      
      # Resource allocation (sharing worker03)
      containers:
      - name: kibana
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"




 oc adm policy add-scc-to-user privileged -z fleet-server -n elastic-observability


====
apiVersion: agent.k8s.elastic.co/v1alpha1
kind: Agent
metadata:
  name: fleet-server
  namespace: elastic-observability
spec:
  version: 9.1.4
  mode: fleet
  fleetServerEnabled: true

  kibanaRef:
    name: observability-kibana
  
  elasticsearchRefs:
  - name: observability-cluster
  
  policyID: eck-fleet-server
  
  deployment:
    replicas: 1
    podTemplate:
      spec:
        nodeSelector:
          kubernetes.io/hostname: "worker03.ocp4.example.com"
        serviceAccountName: fleet-server
        automountServiceAccountToken: true
        # OpenShift-compatible security context that forces privileged SCC
        securityContext:
          runAsUser: 0
          fsGroup: 0

        containers:
        - name: agent
          securityContext:
            runAsUser: 0
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - DAC_OVERRIDE
              - CHOWN
              - FOWNER
              - SETUID
              - SETGID
              - SYS_ADMIN
          resources:
            requests:
              memory: 512Mi
              cpu: 200m
            limits:
              memory: 1Gi
              cpu: 500m
          volumeMounts:
          - name: agent-data
            mountPath: /usr/share/elastic-agent/state
        volumes:
        - name: agent-data
          emptyDir: {}

https://kube-state-metrics.openshift-monitoring.svc.cluster.local:9443